import os
import json
import pickle
from pathlib import Path
from datetime import datetime
from transformers import pipeline
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import hashlib
import re

# –§–∞–π–ª —É–Ω—à–∏–≥—á —Å–∞–Ω–≥—É—É–¥
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("‚ö†Ô∏è PyPDF2 —Å—É—É–≥–∞–∞–≥“Ø–π: pip install PyPDF2")

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("‚ö†Ô∏è python-docx —Å—É—É–≥–∞–∞–≥“Ø–π: pip install python-docx")

try:
    import pandas as pd
    CSV_AVAILABLE = True
except ImportError:
    CSV_AVAILABLE = False
    print("‚ö†Ô∏è pandas —Å—É—É–≥–∞–∞–≥“Ø–π: pip install pandas")

try:
    from pptx import Presentation
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False
    print("‚ö†Ô∏è python-pptx —Å—É—É–≥–∞–∞–≥“Ø–π: pip install python-pptx")

load_dotenv()

# === CONFIG ===
EMBEDDING_MODEL = "sentence-transformers/all-distilroberta-v1"
INDEX_FOLDER = "faiss_disk_index"
METADATA_FILE = "disk_metadata.pkl"
SUPPORTED_EXTENSIONS = ['.txt', '.pdf', '.docx', '.doc', '.json', '.jsonl', '.csv', '.md', '.pptx', '.ppt']

class AdvancedDiskSearch:
    def __init__(self, search_paths=None):
        self.search_paths = search_paths or ["D:/", "C:/Users"]
        self.embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        self.db = None
        self.metadata = {}
        self.all_docs = []
        
        # AI —Å–æ–Ω–≥–æ–ª—Ç
        print("\nüí° AI —Ö–∞—Ä–∏—É–ª—Ç “Ø“Ø—Å–≥—ç—Ö –≥–æ—Ä–∏–º—ã–≥ —Å–æ–Ω–≥–æ–Ω–æ —É—É:")
        print("   1) –ò–¥—ç–≤—Ö–≥“Ø–π (—à—É—É–¥ –∞–≥—É—É–ª–≥–∞ —Ö–∞—Ä—É—É–ª–∞—Ö)")
        print("   2) –°—É—É—Ä—å AI (flan-t5-base, —Ö—É—Ä–¥–∞–Ω)")
        print("   3) –î—ç–≤—à–∏–ª—Ç—ç—Ç AI (flan-t5-large, –∏–ª“Ø“Ø —Å–∞–π–Ω)")
        ai_choice = input("\n   –°–æ–Ω–≥–æ–ª—Ç (1/2/3, default=1): ").strip() or "1"
        
        self.pipe = None
        self.use_ai = False
        
        if ai_choice in ['2', '3']:
            try:
                model_name = "google/flan-t5-large" if ai_choice == '3' else "google/flan-t5-base"
                print(f"\nüîÑ AI model –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞ ({model_name})...")
                if ai_choice == '3':
                    print("   ‚ö†Ô∏è  –ê–Ω—Ö —É–¥–∞–∞ –±–æ–ª 1-2 –º–∏–Ω—É—Ç “Ø—Ä–≥—ç–ª–∂–∏–ª–Ω—ç...")
                
                self.pipe = pipeline(
                    "text2text-generation",
                    model=model_name,
                    max_new_tokens=200,
                    device=-1
                )
                self.use_ai = True
                print("‚úÖ AI model –±—ç–ª—ç–Ω –±–æ–ª–ª–æ–æ\n")
            except Exception as e:
                print(f"‚ö†Ô∏è AI model –∞–ª–¥–∞–∞: {e}")
                print("   –§–∞–π–ª—ã–Ω –∞–≥—É—É–ª–≥—ã–≥ —à—É—É–¥ —Ö–∞—Ä—É—É–ª–∞—Ö –≥–æ—Ä–∏–º–¥ —à–∏–ª–∂–ª—ç—ç\n")
        else:
            print("‚úÖ –§–∞–π–ª—ã–Ω –∞–≥—É—É–ª–≥—ã–≥ —à—É—É–¥ —Ö–∞—Ä—É—É–ª–∞—Ö –≥–æ—Ä–∏–º\n")

    def get_file_hash(self, filepath):
        try:
            with open(filepath, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return None

    def should_skip_directory(self, dirpath):
        skip_dirs = {
            'node_modules', '__pycache__', '.git', '.venv', 'venv',
            'AppData', 'Windows', 'Program Files', 'System32',
            '$RECYCLE.BIN', 'Recovery', 'ProgramData',
            'Microsoft VS Code', 'Visual Studio', 'extensions',
            'resources', 'locales', 'vendor', 'build', 'dist'
        }
        return any(skip in dirpath for skip in skip_dirs)

    def read_txt_file(self, filepath):
        encodings = ['utf-8', 'utf-16', 'cp1252', 'latin-1']
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    return f.read()
            except:
                continue
        return None

    def read_pdf_file(self, filepath):
        if not PDF_AVAILABLE:
            return None
        try:
            text = ""
            with open(filepath, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages[:50]:  # Limit to 50 pages
                    extracted = page.extract_text()
                    if extracted:
                        text += extracted + "\n"
            return text.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è PDF —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_docx_file(self, filepath):
        if not DOCX_AVAILABLE:
            return None
        try:
            doc = docx.Document(filepath)
            text = "\n".join([para.text for para in doc.paragraphs])
            return text.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è DOCX —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_csv_file(self, filepath):
        if not CSV_AVAILABLE:
            return None
        try:
            df = pd.read_csv(filepath, encoding='utf-8', nrows=1000)
            return df.to_string()
        except Exception as e:
            try:
                df = pd.read_csv(filepath, encoding='latin-1', nrows=1000)
                return df.to_string()
            except:
                print(f"‚ö†Ô∏è CSV —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
                return None

    def read_json_file(self, filepath):
        try:
            if filepath.endswith('.jsonl'):
                texts = []
                with open(filepath, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= 100:
                            break
                        data = json.loads(line)
                        texts.append(json.dumps(data, indent=2, ensure_ascii=False))
                return "\n---\n".join(texts)
            else:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return json.dumps(data, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"‚ö†Ô∏è JSON —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_pptx_file(self, filepath):
        if not PPTX_AVAILABLE:
            return None
        try:
            prs = Presentation(filepath)
            text = []
            for slide in prs.slides[:50]:  # Limit to 50 slides
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        text.append(shape.text)
            return "\n".join(text)
        except Exception as e:
            print(f"‚ö†Ô∏è PPTX —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_file(self, filepath):
        ext = Path(filepath).suffix.lower()
        if ext in ['.txt', '.md', '.log']:
            return self.read_txt_file(filepath)
        elif ext == '.pdf':
            return self.read_pdf_file(filepath)
        elif ext in ['.docx', '.doc']:
            return self.read_docx_file(filepath)
        elif ext == '.csv':
            return self.read_csv_file(filepath)
        elif ext in ['.json', '.jsonl']:
            return self.read_json_file(filepath)
        elif ext in ['.pptx', '.ppt']:
            return self.read_pptx_file(filepath)
        else:
            return None

    def scan_disk(self, max_files=1000, max_size_mb=10):
        print(f"üîç –î–∏—Å–∫ —Ö–∞–π–∂ —ç—Ö—ç–ª–∂ –±–∞–π–Ω–∞: {self.search_paths}")
        print(f"üìÇ –î—ç–º–∂–∏–≥–¥—Å—ç–Ω —Ñ–∞–π–ª—ã–Ω —Ç”©—Ä”©–ª: {', '.join(SUPPORTED_EXTENSIONS)}")
        documents = []
        file_count = 0
        max_size_bytes = max_size_mb * 1024 * 1024
        for search_path in self.search_paths:
            if not os.path.exists(search_path):
                print(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏ –æ–ª–¥—Å–æ–Ω–≥“Ø–π: {search_path}")
                continue
            print(f"\nüìÅ –•–∞–π–∂ –±–∞–π–Ω–∞: {search_path}")
            for root, dirs, files in os.walk(search_path):
                if self.should_skip_directory(root):
                    dirs[:] = []
                    continue
                for filename in files:
                    if file_count >= max_files:
                        print(f"\n‚ö†Ô∏è –•—è–∑–≥–∞–∞—Ä—Ç —Ö“Ø—Ä–ª—ç—ç: {max_files} —Ñ–∞–π–ª")
                        break
                    filepath = os.path.join(root, filename)
                    ext = Path(filename).suffix.lower()
                    if ext not in SUPPORTED_EXTENSIONS:
                        continue
                    try:
                        file_size = os.path.getsize(filepath)
                        if file_size > max_size_bytes or file_size == 0:
                            continue
                    except:
                        continue
                    content = self.read_file(filepath)
                    if not content or len(content.strip()) < 50:
                        continue
                    file_hash = self.get_file_hash(filepath)
                    modified_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                    doc = Document(
                        page_content=content[:5000],
                        metadata={
                            "filename": filename,
                            "filepath": filepath,
                            "extension": ext,
                            "size_kb": file_size / 1024,
                            "modified": modified_time.isoformat(),
                            "hash": file_hash
                        }
                    )
                    documents.append(doc)
                    file_count += 1
                    if file_count % 50 == 0:
                        print(f"‚úÖ {file_count} —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω...")
                if file_count >= max_files:
                    break
        print(f"\n‚úÖ –ù–∏–π—Ç {len(documents)} —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω")
        self.all_docs = documents
        return documents

    def create_index(self, documents):
        if not documents:
            print("‚ùå –ò–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç—Ö –±–∞—Ä–∏–º—Ç –±–∞–π—Ö–≥“Ø–π")
            return False
        print(f"\nüîÑ FAISS –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞ ({len(documents)} –±–∞—Ä–∏–º—Ç)...")
        try:
            self.db = FAISS.from_documents(documents, self.embedding)
            os.makedirs(INDEX_FOLDER, exist_ok=True)
            self.db.save_local(INDEX_FOLDER)
            self.metadata = {
                "created": datetime.now().isoformat(),
                "num_documents": len(documents),
                "files": [doc.metadata for doc in documents]
            }
            with open(METADATA_FILE, 'wb') as f:
                pickle.dump(self.metadata, f)
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∞–º–∂–∏–ª—Ç—Ç–∞–π “Ø“Ø—Å–≥—ç–≥–¥–ª—ç—ç: {INDEX_FOLDER}")
            return True
        except Exception as e:
            print(f"‚ùå –ò–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç—Ö—ç–¥ –∞–ª–¥–∞–∞: {e}")
            return False

    def load_index(self):
        if not os.path.exists(INDEX_FOLDER):
            print("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –æ–ª–¥—Å–æ–Ω–≥“Ø–π. –≠—Ö–ª—ç—ç–¥ scan_disk() –¥—É—É–¥–Ω–∞ —É—É.")
            return False
        try:
            print("üîÑ –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞...")
            self.db = FAISS.load_local(INDEX_FOLDER, self.embedding, allow_dangerous_deserialization=True)
            if os.path.exists(METADATA_FILE):
                with open(METADATA_FILE, 'rb') as f:
                    self.metadata = pickle.load(f)
                print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞–≥–¥–ª–∞–∞: {self.metadata.get('num_documents', 0)} –±–∞—Ä–∏–º—Ç")
            else:
                print("‚ö†Ô∏è Metadata –æ–ª–¥—Å–æ–Ω–≥“Ø–π")
            return True
        except Exception as e:
            print(f"‚ùå –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞—Ö–∞–¥ –∞–ª–¥–∞–∞: {e}")
            return False

    def search_by_keyword(self, keyword):
        keyword = keyword.lower()
        results = []
        if self.metadata and "files" in self.metadata:
            for file_meta in self.metadata["files"]:
                if keyword in file_meta.get("filename", "").lower():
                    results.append(file_meta)
        return results

    def semantic_search(self, query, k=5, score_threshold=2.0):
        if not self.db:
            print("‚ùå –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞–∞–≥“Ø–π –±–∞–π–Ω–∞")
            return []
        try:
            results = self.db.similarity_search_with_score(query, k=k)
            filtered = [(doc, score) for doc, score in results if score < score_threshold]
            if not filtered and results:
                print(f"‚ö†Ô∏è Threshold-–æ–æ—Å –¥–∞–≤—Å–∞–Ω, –±“Ø—Ö “Ø—Ä –¥“Ø–Ω–≥ —Ö–∞—Ä—É—É–ª–∂ –±–∞–π–Ω–∞")
                filtered = results
            return filtered
        except Exception as e:
            print(f"‚ùå –•–∞–π–ª—Ç–∞–¥ –∞–ª–¥–∞–∞: {e}")
            return []

    def extract_smart_info(self, content, query):
        """
        –ï—Ä”©–Ω—Ö–∏–π –∑–æ—Ä–∏—É–ª–∞–ª—Ç—ã–Ω —É—Ö–∞–∞–ª–∞–≥ –º—ç–¥—ç—ç–ª—ç–ª –∑–∞–¥–ª–∞—Ö
        –Ø–º–∞—Ä —á —Ç”©—Ä–ª–∏–π–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π
        """
        extracted = []
        query_lower = query.lower()
        query_words = set(re.findall(r'\w+', query_lower))
        
        # === 1. –¢“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥-—É—Ç–≥–∞ —Ö–æ—Å –æ–ª–æ—Ö (Key: Value, Key = Value, Key - Value) ===
        kv_patterns = [
            r'([A-Za-z–ê-–Ø–∞-—è—ë“Ø”©–Å“Æ”®\w\s]{2,30})\s*[:\Ôºö=]\s*([^\n\r]{3,100})',
            r'([A-Za-z–ê-–Ø–∞-—è—ë“Ø”©–Å“Æ”®\w\s]{2,30})\s*[-‚Äì‚Äî]\s*([^\n\r]{3,100})',
        ]
        
        for pattern in kv_patterns:
            matches = re.findall(pattern, content)
            for key, value in matches:
                key_clean = key.strip().lower()
                value_clean = value.strip()
                # –ê—Å—É—É–ª—Ç—ã–Ω “Ø–≥—Ç—ç–π —Ö–æ–ª–±–æ–æ—Ç–æ–π —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞—Ö
                key_words = set(re.findall(r'\w+', key_clean))
                if query_words & key_words or any(qw in key_clean for qw in query_words if len(qw) > 2):
                    if len(value_clean) > 2 and len(value_clean) < 200:
                        extracted.append(f"‚úì {key.strip()}: {value_clean}")
        
        # === 2. –ê—Å—É—É–ª—Ç—ã–Ω —Ç“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥–∏–π–Ω —ç—Ä–≥—ç–Ω —Ç–æ–π—Ä–æ–Ω –¥–∞—Ö—å –∫–æ–Ω—Ç–µ–∫—Å—Ç ===
        content_lower = content.lower()
        for word in query_words:
            if len(word) < 3:
                continue
            # –¢“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥ –æ–ª–æ—Ö
            for match in re.finditer(re.escape(word), content_lower):
                start = max(0, match.start() - 100)
                end = min(len(content), match.end() + 150)
                context = content[start:end].strip()
                
                # –ú”©—Ä –±“Ø—Ç–Ω—ç—ç—Ä –∞–≤–∞—Ö
                lines = context.split('\n')
                relevant_lines = []
                for line in lines:
                    if word in line.lower() and len(line.strip()) > 10:
                        clean_line = line.strip()
                        if clean_line not in [e.split(': ', 1)[-1] if ': ' in e else e for e in extracted]:
                            relevant_lines.append(clean_line)
                
                if relevant_lines and len(extracted) < 5:
                    for line in relevant_lines[:2]:
                        if len(line) < 200:
                            extracted.append(f"‚Üí {line}")
                break  # –ó”©–≤—Ö”©–Ω —ç—Ö–Ω–∏–π —Ç–æ—Ö–∏—Ä–ª—ã–≥ –∞–≤–∞—Ö
        
        # === 3. –¢–æ–æ–Ω —É—Ç–≥–∞, —Ö—É–≤—å, —Ö—ç–º–∂—ç—ç –æ–ª–æ—Ö ===
        number_context_pattern = r'([A-Za-z–ê-–Ø–∞-—è—ë“Ø”©–Å“Æ”®\w\s]{2,25})\s*[:\Ôºö]?\s*(\d+(?:[.,]\d+)?)\s*(%|—Ö—É–≤—å|percent|USD|‚ÇÆ|¬•|\$|‚Ç¨|kg|km|–º|cm|mm|GB|MB|TB|‰ª∂|ÂÄã|‰∫∫|Âπ¥|Êúà|Êó•)?'
        num_matches = re.findall(number_context_pattern, content)
        for label, number, unit in num_matches:
            label_clean = label.strip().lower()
            if any(qw in label_clean for qw in query_words if len(qw) > 2):
                unit_str = unit if unit else ""
                extracted.append(f"‚úì {label.strip()}: {number}{unit_str}")
        
        # === 4. –û–≥–Ω–æ–æ, —Ö—É–≥–∞—Ü–∞–∞ –æ–ª–æ—Ö ===
        date_patterns = [
            r'(\d{4})\s*[Âπ¥/-]\s*(\d{1,2})\s*[Êúà/-]?\s*(\d{1,2})?\s*Êó•?',
            r'(\d{1,2})[/.-](\d{1,2})[/.-](\d{2,4})',
        ]
        for pattern in date_patterns:
            date_matches = re.findall(pattern, content)
            if date_matches and any(kw in query_lower for kw in ['when', 'date', 'time', '—Ö—ç–∑—ç—ç', '–æ–≥–Ω–æ–æ', 'year', 'month', '–æ–Ω', '—Å–∞—Ä']):
                for dm in date_matches[:3]:
                    date_str = "/".join([d for d in dm if d])
                    if date_str not in str(extracted):
                        extracted.append(f"üìÖ {date_str}")
        
        # === 5. Email, Phone, URL - –µ—Ä”©–Ω—Ö–∏–π ===
        common_patterns = {
            'Email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            'Phone': r'[\+]?[\d\s\-\(\)]{8,15}',
            'URL': r'https?://[^\s<>"{}|\\^`\[\]]+',
        }
        for label, pattern in common_patterns.items():
            if any(kw in query_lower for kw in [label.lower(), 'contact', '—Ö–æ–ª–±–æ–æ', '—É—Ç–∞—Å', '–∏–º—ç–π–ª', 'link', 'website']):
                matches = re.findall(pattern, content)
                if matches:
                    extracted.append(f"‚úì {label}: {matches[0]}")
        
        # === 6. –ñ–∞–≥—Å–∞–∞–ª—Ç, bullet point –æ–ª–æ—Ö ===
        list_patterns = [
            r'[‚Ä¢¬∑‚ó¶‚ñ™‚ñ´‚óè‚óã]\s*([^\n]{5,100})',
            r'^\s*[-\*]\s+([^\n]{5,100})',
            r'^\s*\d+[.)]\s+([^\n]{5,100})',
        ]
        for pattern in list_patterns:
            list_matches = re.findall(pattern, content, re.MULTILINE)
            relevant_items = [item.strip() for item in list_matches 
                           if any(qw in item.lower() for qw in query_words if len(qw) > 2)]
            if relevant_items and len(extracted) < 8:
                for item in relevant_items[:3]:
                    extracted.append(f"‚Ä¢ {item}")
        
        # === 7. –î–∞–≤—Ö–∞—Ä–¥–∞–ª –∞—Ä–∏–ª–≥–∞—Ö ===
        unique_extracted = []
        seen = set()
        for item in extracted:
            # –•—è–ª–±–∞—Ä—à—É—É–ª—Å–∞–Ω —Ç–µ–∫—Å—Ç—ç—ç—Ä –¥–∞–≤—Ö–∞—Ä–¥–∞–ª —à–∞–ª–≥–∞—Ö
            simplified = re.sub(r'[^\w\s]', '', item.lower())
            if simplified not in seen and len(simplified) > 5:
                seen.add(simplified)
                unique_extracted.append(item)
        
        return unique_extracted[:8] if unique_extracted else None

    def generate_answer(self, results, question):
        """–°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω AI —Ö–∞—Ä–∏—É–ª—Ç “Ø“Ø—Å–≥—ç—Ö - –æ–ª–æ–Ω —Ç”©—Ä–ª–∏–π–Ω –∞—Å—É—É–ª—Ç–∞–¥"""
        if not results:
            return "–û–ª–¥—Å–æ–Ω —Ñ–∞–π–ª—É—É–¥–∞–¥ —Ö–∞—Ä–∏—É–ª—Ç –±–∞–π—Ö–≥“Ø–π."

        # 1. –≠—Ö–ª—ç—ç–¥ —É—Ö–∞–∞–ª–∞–≥ –∑–∞–¥–ª–∞–ª—Ç —Ö–∏–π—Ö (pattern matching)
        all_extracted = []
        sources_with_info = []
        
        for doc, score in results[:5]:  # Top 5 –±–∞—Ä–∏–º—Ç —à–∞–ª–≥–∞—Ö
            extracted = self.extract_smart_info(doc.page_content, question)
            if extracted:
                filename = doc.metadata.get('filename', 'Unknown')
                for info in extracted:
                    all_extracted.append(info)
                    sources_with_info.append(filename)
        
        # –•—ç—Ä—ç–≤ pattern-—ç—ç—Ä –æ–ª–¥–≤–æ–ª —à—É—É–¥ –±—É—Ü–∞–∞—Ö
        if all_extracted:
            unique_sources = list(set(sources_with_info))
            result = "\n".join(all_extracted)
            result += f"\n\nüìö –≠—Ö: {', '.join(unique_sources)}"
            return result
        
        # 2. Pattern –æ–ª–¥–æ—Ö–≥“Ø–π –±–æ–ª AI –∞—à–∏–≥–ª–∞—Ö
        if not self.use_ai:
            # AI –∏–¥—ç–≤—Ö–≥“Ø–π –±–æ–ª –∞–≥—É—É–ª–≥—ã–Ω —Ö—É—Ä–∞–∞–Ω–≥—É–π –±—É—Ü–∞–∞—Ö
            summaries = []
            for i, (doc, score) in enumerate(results[:2], 1):
                filename = doc.metadata.get('filename', 'Unknown')
                snippet = doc.page_content[:400].strip()
                summaries.append(f"üìÑ [{i}] {filename}:\n{snippet}...")
            return "\n\n".join(summaries)
        
        # 3. AI —Ö–∞—Ä–∏—É–ª—Ç “Ø“Ø—Å–≥—ç—Ö
        snippets = []
        sources = []
        
        for i, (doc, score) in enumerate(results[:3], 1):
            filename = doc.metadata.get("filename", f"source_{i}")
            content = doc.page_content.replace("\n", " ").strip()
            snippet = content[:350]
            snippets.append(f"[{i}] {filename}: {snippet}")
            sources.append(filename)

        context = "\n\n".join(snippets)
        
        # –¢–æ–≤—á, —Ç–æ–¥–æ—Ä—Ö–æ–π prompt
        prompt = (
            f"Based on these documents, answer the question directly and concisely.\n"
            f"Extract specific facts, dates, names, or numbers if present.\n\n"
            f"{context}\n\n"
            f"Question: {question}\n"
            f"Answer (be brief and specific):"
        )
        
        # Token —à–∞–ª–≥–∞–ª—Ç
        if len(prompt.split()) > 400:
            snippets = [s[:250] for s in snippets[:2]]
            context = "\n".join(snippets)
            prompt = f"Answer based on:\n{context}\n\nQ: {question}\nA:"

        try:
            result = self.pipe(
                prompt,
                max_new_tokens=120,
                do_sample=False,
                num_beams=1,
                early_stopping=True
            )
            
            text = ""
            if isinstance(result, list) and len(result) > 0:
                text = result[0].get("generated_text", "") or result[0].get("summary_text", "")
            
            # –•–∞—Ä–∏—É–ª—Ç —à–∞–ª–≥–∞—Ö
            if not text or len(text.strip()) < 10:
                return f"üìã –§–∞–π–ª—É—É–¥–∞–¥ –º—ç–¥—ç—ç–ª—ç–ª –±–∞–π–≥–∞–∞ –±–æ–ª–æ–≤—á AI –∑–∞–¥–ª–∞–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π.\n\nüìö –≠—Ö: {', '.join(sources)}"
            
            text = text.strip()
            
            # –î–∞–≤—Ç–∞–ª—Ç —à–∞–ª–≥–∞—Ö (AI –∞–ª–¥–∞–∞)
            words = text.split()
            if len(words) > 3:
                # –ù—ç–≥ “Ø–≥ —Ö—ç—Ç –∏—Ö –¥–∞–≤—Ç–∞–≥–¥–∞–∂ –±–∞–π–≤–∞–ª
                word_counts = {}
                for word in words:
                    word_counts[word] = word_counts.get(word, 0) + 1
                max_count = max(word_counts.values())
                if max_count > len(words) / 3:
                    return f"‚ö†Ô∏è AI —Ö–∞—Ä–∏—É–ª—Ç –∞–ª–¥–∞–∞—Ç–∞–π (–¥–∞–≤—Ç–∞–ª—Ç –∏–ª—ç—Ä—Å—ç–Ω)\n\nüìö –≠—Ö: {', '.join(sources)}"
            
            # Prompt –¥–∞–≤—Ç–∞–ª—Ç —à–∞–ª–≥–∞—Ö
            if "Question:" in text or "Answer:" in text:
                # Prompt-—ã–≥ –¥–∞–≤—Ç–∞–∂ –±—É—Ü–∞–∞—Å–∞–Ω
                parts = text.split("Answer:")
                if len(parts) > 1:
                    text = parts[-1].strip()
            
            # –≠—Ö —Å—É—Ä–≤–∞–ª–∂ –Ω—ç–º—ç—Ö
            if not any(s in text for s in sources):
                text += f"\n\nüìö –≠—Ö: {', '.join(sources)}"
            
            return text
            
        except Exception as e:
            return f"‚ö†Ô∏è AI –∞–ª–¥–∞–∞: {e}\n\nüìö –≠—Ö: {', '.join(sources)}"

    def interactive_search(self):
        print("\n" + "="*60)
        print("üß† –î–∏—Å–∫ –•–∞–π–ª—Ç—ã–Ω AI –°–∏—Å—Ç–µ–º –ë—ç–ª—ç–Ω")
        print("="*60)
        print("üí° –ö–æ–º–∞–Ω–¥—É—É–¥:")
        print("  - 'stats' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ —Ö–∞—Ä–∞—Ö")
        print("  - 'rescan' - –¥–∞—Ö–∏–Ω —Ö–∞–π—Ö")
        print("  - 'exit' - –≥–∞—Ä–∞—Ö")
        print("="*60 + "\n")

        while True:
            user_input = input("üîç –ê—Å—É—É–ª—Ç: ").strip()
            if not user_input:
                continue
            if user_input.lower() == "exit":
                print("üëã –ë–∞—è—Ä—Ç–∞–π!")
                break
            if user_input.lower() == "stats":
                self.show_statistics()
                continue
            if user_input.lower() == "rescan":
                print("\nüîÑ –î–∞—Ö–∏–Ω —Ö–∞–π–∂, –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
                docs = self.scan_disk(max_files=500)
                self.create_index(docs)
                continue

            # –¢“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥—ç—ç—Ä —Ö–∞–π–ª—Ç
            keyword_results = self.search_by_keyword(user_input)
            if keyword_results:
                print(f"\n[üîé –¢“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥] {len(keyword_results)} —Ñ–∞–π–ª –æ–ª–¥–ª–æ–æ:")
                for result in keyword_results[:3]:
                    print(f"  üìÑ {result['filename']} ({result['extension']})")
                    print(f"     üìÅ {result['filepath']}")

            # –£—Ç–≥—ã–Ω —Ö–∞–π–ª—Ç
            semantic_results = self.semantic_search(user_input, k=5)
            if not semantic_results:
                print("\n‚ùå –•–æ–ª–±–æ–≥–¥–æ—Ö –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
                print("üí° –ó”©–≤–ª”©–º–∂:")
                print("   - ”®”©—Ä “Ø–≥ —Ö—ç–ª–ª—ç–≥—ç—ç—Ä –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É")
                print("   - 'stats' –∫–æ–º–∞–Ω–¥–∞–∞—Ä —è–º–∞—Ä —Ñ–∞–π–ª—É—É–¥ –±–∞–π–≥–∞–∞–≥ —Ö–∞—Ä–Ω–∞ —É—É")
                print("   - 'rescan' –∫–æ–º–∞–Ω–¥–∞–∞—Ä –¥–∞—Ö–∏–Ω —Ö–∞–π–∂ “Ø–∑–Ω—ç “Ø“Ø\n")
                continue

            print(f"\n[üìö –£—Ç–≥—ã–Ω —Ö–∞–π–ª—Ç] {len(semantic_results)} –±–∞—Ä–∏–º—Ç –æ–ª–¥–ª–æ–æ:")
            for i, (doc, score) in enumerate(semantic_results, 1):
                snippet = doc.page_content[:200].replace("\n", " ") + "..."
                print(f"\n{i}. üéØ –û–Ω–æ–æ: {score:.4f}")
                print(f"   üìÑ –§–∞–π–ª: {doc.metadata.get('filename', 'Unknown')}")
                print(f"   üìÇ –ó–∞–º: {doc.metadata.get('filepath', 'Unknown')}")
                print(f"   üìè –•—ç–º–∂—ç—ç: {doc.metadata.get('size_kb', 0):.1f} KB")
                print(f"   üìù –ê–≥—É—É–ª–≥–∞: {snippet}")

            # AI —Ö–∞—Ä–∏—É–ª—Ç
            if self.use_ai:
                print(f"\nü§ñ AI —Ö–∞—Ä–∏—É–ª—Ç “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
                answer = self.generate_answer(semantic_results[:3], user_input)
                print(f"\nüí° AI –•–∞—Ä–∏—É–ª—Ç:")
                print("="*60)
                print(answer)
                print("="*60)
            
            # –§–∞–π–ª—ã–Ω –∞–≥—É—É–ª–≥–∞ “Ø—Ä–≥—ç–ª–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
            print(f"\nüìã –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –∞–≥—É—É–ª–≥–∞:")
            for i, (doc, score) in enumerate(semantic_results[:2], 1):
                content = doc.page_content[:600].strip()
                print(f"\n{'‚îÄ'*60}")
                print(f"[{i}] {doc.metadata.get('filename')}")
                print(f"{'‚îÄ'*60}")
                print(content)
                if len(doc.page_content) > 600:
                    print(f"... ({len(doc.page_content)-600} —Ç—ç–º–¥—ç–≥—Ç “Ø–ª–¥—Å—ç–Ω)")
            
            print("\n" + "-"*60 + "\n")

    def show_statistics(self):
        if not self.metadata:
            print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫ –±–∞–π—Ö–≥“Ø–π")
            return
        print("\n" + "="*60)
        print("üìä –°–∏—Å—Ç–µ–º–∏–π–Ω –°—Ç–∞—Ç–∏—Å—Ç–∏–∫")
        print("="*60)
        print(f"üìÖ “Æ“Ø—Å—Å—ç–Ω: {self.metadata.get('created', 'Unknown')}")
        print(f"üìÑ –ù–∏–π—Ç –±–∞—Ä–∏–º—Ç: {self.metadata.get('num_documents', 0)}")
        if "files" in self.metadata:
            extensions = {}
            total_size = 0
            for file_meta in self.metadata["files"]:
                ext = file_meta.get("extension", "unknown")
                size = file_meta.get("size_kb", 0)
                extensions[ext] = extensions.get(ext, 0) + 1
                total_size += size
            print(f"üíæ –ù–∏–π—Ç —Ö—ç–º–∂—ç—ç: {total_size/1024:.2f} MB")
            print("\nüìÇ –§–∞–π–ª—ã–Ω —Ç”©—Ä”©–ª:")
            for ext, count in sorted(extensions.items(), key=lambda x: x[1], reverse=True):
                print(f"   {ext}: {count} —Ñ–∞–π–ª")
            print("\nüìã –ë–∞—Ä–∏–º—Ç—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç:")
            for i, file_meta in enumerate(self.metadata["files"][:20], 1):
                print(f"   {i}. {file_meta.get('filename', 'Unknown')} ({file_meta.get('extension', 'unknown')})")
            if len(self.metadata["files"]) > 20:
                print(f"   ... –±–æ–ª–æ–Ω ”©”©—Ä {len(self.metadata['files']) - 20} —Ñ–∞–π–ª")
        print("="*60 + "\n")

def main():
    print("üöÄ –î–∏—Å–∫ –•–∞–π–ª—Ç—ã–Ω –°–∏—Å—Ç–µ–º –≠—Ö—ç–ª–ª—ç—ç\n")
    print("üìÅ –•–∞–π—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—É—É–¥—ã–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É (—Ç–∞—Å–ª–∞–ª–∞–∞—Ä —Ç—É—Å–≥–∞–∞—Ä–ª–∞–Ω–∞):")
    print("   –ñ–∏—à—ç—ç: D:/Documents, D:/Projects, C:/Users/YourName/Desktop")
    print("   –•–æ–æ—Å–æ–Ω –æ—Ä—Ö–∏–≤–æ–ª D:/ –±–æ–ª–æ–Ω C:/Users —Ö–∞–π–Ω–∞")
    user_paths = input("\nüìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏: ").strip()
    if user_paths:
        search_paths = [p.strip() for p in user_paths.split(",")]
    else:
        search_paths = ["D:/", "C:/Users"]
    searcher = AdvancedDiskSearch(search_paths=search_paths)
    if os.path.exists(INDEX_FOLDER):
        print(f"\n‚úÖ –•–∞–¥–≥–∞–ª—Å–∞–Ω –∏–Ω–¥–µ–∫—Å –æ–ª–¥–ª–æ–æ: {INDEX_FOLDER}")
        choice = input("–ê—à–∏–≥–ª–∞—Ö —É—É? (y/n): ").strip().lower()
        if choice == 'y':
            if searcher.load_index():
                searcher.interactive_search()
                return
    print("\nüîÑ –î–∏—Å–∫ —Ö–∞–π–∂, –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
    print("‚ö†Ô∏è –≠–Ω—ç —É–¥–∞–∞–Ω “Ø—Ä–≥—ç–ª–∂–ª—ç—Ö –±–æ–ª–æ–º–∂—Ç–æ–π (5-30 –º–∏–Ω—É—Ç)")
    max_files = input("\n–•–∞–º–≥–∏–π–Ω –∏—Ö —Ñ–∞–π–ª—ã–Ω —Ç–æ–æ (default 1000): ").strip()
    max_files = int(max_files) if max_files.isdigit() else 1000
    docs = searcher.scan_disk(max_files=max_files, max_size_mb=10)
    if docs:
        if searcher.create_index(docs):
            searcher.interactive_search()
    else:
        print("‚ùå –§–∞–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π")

if __name__ == "__main__":
    main() 