"""
RAG Disk Search ‚Äî –û—Ñ–ª–∞–π–Ω, CPU-–¥ –∑–æ—Ä–∏—É–ª—Å–∞–Ω
–§–∞–π–ª—É—É–¥—ã–≥ chunk-–ª—ç–∂ –∏–Ω–¥–µ–∫—Å–ª—ç—ç–¥, –∞—Å—É—É–ª—Ç–∞–¥ AI-—Ä —Ö–∞—Ä–∏—É–ª–Ω–∞.
Ollama (“Ø–Ω–¥—Å—ç–Ω) —ç—Å–≤—ç–ª flan-t5 (–Ω”©”©—Ü) –∞—à–∏–≥–ª–∞–Ω–∞.
"""
import os
import json
import pickle
import hashlib
import requests
from pathlib import Path
from datetime import datetime

from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# –§–∞–π–ª —É–Ω—à–∏–≥—á —Å–∞–Ω–≥—É—É–¥ (optional)
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import pandas as pd
    CSV_AVAILABLE = True
except ImportError:
    CSV_AVAILABLE = False

try:
    from pptx import Presentation
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False

load_dotenv()

# === CONFIG ===
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
INDEX_FOLDER = "faiss_rag_index"
METADATA_FILE = "rag_metadata.pkl"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
SUPPORTED_EXTENSIONS = {
    '.txt', '.pdf', '.docx', '.doc', '.json', '.jsonl',
    '.csv', '.md', '.pptx', '.ppt', '.log',
}

OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "qwen2"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  Ollama LLM ‚Äî –æ—Ñ–ª–∞–π–Ω, CPU
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
class OllamaLLM:
    def __init__(self, model=OLLAMA_MODEL, base_url=OLLAMA_URL):
        self.model = model
        self.base_url = base_url

    def is_available(self):
        try:
            return requests.get(f"{self.base_url}/api/tags", timeout=3).status_code == 200
        except Exception:
            return False

    def list_models(self):
        try:
            r = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if r.status_code == 200:
                return [m["name"] for m in r.json().get("models", [])]
        except Exception:
            pass
        return []

    def generate_stream(self, prompt, temperature=0.3):
        try:
            r = requests.post(
                f"{self.base_url}/api/generate",
                json={"model": self.model, "prompt": prompt, "stream": True,
                      "options": {"temperature": temperature}},
                stream=True, timeout=300,
            )
            for line in r.iter_lines():
                if line:
                    data = json.loads(line)
                    if "response" in data:
                        yield data["response"]
                    if data.get("done"):
                        break
        except requests.exceptions.ConnectionError:
            yield "\n‚ùå Ollama —Ö–æ–ª–±–æ–≥–¥—Å–æ–Ω–≥“Ø–π. 'ollama serve' –∞–∂–∏–ª–ª—É—É–ª–Ω–∞ —É—É."
        except Exception as e:
            yield f"\n‚ùå –ê–ª–¥–∞–∞: {e}"

    def generate(self, prompt, temperature=0.3, max_tokens=512):
        try:
            r = requests.post(
                f"{self.base_url}/api/generate",
                json={"model": self.model, "prompt": prompt, "stream": False,
                      "options": {"temperature": temperature, "num_predict": max_tokens}},
                timeout=300,
            )
            if r.status_code == 200:
                return r.json().get("response", "").strip()
            return f"Ollama –∞–ª–¥–∞–∞: {r.status_code}"
        except Exception as e:
            return f"‚ùå –ê–ª–¥–∞–∞: {e}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  RAG —Å–∏—Å—Ç–µ–º
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
class DiskSearchRAG:
    def __init__(self, search_paths=None):
        self.search_paths = search_paths or []
        self.db = None
        self.metadata = {}

        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

        print(f"üîÑ Embedding –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞ ({EMBEDDING_MODEL})...")
        self.embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        print("‚úÖ Embedding –±—ç–ª—ç–Ω\n")

        # AI model –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —Å–æ–Ω–≥–æ—Ö
        self.llm = None
        self.pipe = None
        self.ai_mode = "none"
        self._init_ai()

    # ‚îÄ‚îÄ AI model —Å–æ–Ω–≥–æ—Ö ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _init_ai(self):
        ollama = OllamaLLM()
        if ollama.is_available():
            models = ollama.list_models()
            if models:
                names = [m.split(":")[0] for m in models]
                if OLLAMA_MODEL in names or OLLAMA_MODEL.split(":")[0] in names:
                    ollama.model = OLLAMA_MODEL
                else:
                    ollama.model = models[0]
                self.llm = ollama
                self.ai_mode = "ollama"
                print(f"‚úÖ Ollama –±—ç–ª—ç–Ω ‚Äî model: {ollama.model}")
                if len(models) > 1:
                    print(f"   –ë—É—Å–∞–¥: {', '.join(models[:5])}")
                return

        print("‚ö†Ô∏è Ollama –æ–ª–¥—Å–æ–Ω–≥“Ø–π ‚Üí flan-t5-base –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞...")
        try:
            from transformers import pipeline
            self.pipe = pipeline(
                "text2text-generation",
                model="google/flan-t5-base",
                max_new_tokens=200,
                device=-1,
            )
            self.ai_mode = "flan-t5"
            print("‚úÖ flan-t5-base –±—ç–ª—ç–Ω\n")
        except Exception as e:
            print(f"‚ö†Ô∏è AI model –∞—á–∞–∞–ª–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π: {e}")
            self.ai_mode = "none"

    # ‚îÄ‚îÄ –§–∞–π–ª —É–Ω—à–∏–≥—á–∏–¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @staticmethod
    def _read_txt(filepath):
        for enc in ("utf-8", "utf-16", "cp1252", "latin-1"):
            try:
                with open(filepath, "r", encoding=enc, errors="ignore") as f:
                    return f.read(200_000)
            except Exception:
                continue
        return None

    @staticmethod
    def _read_pdf(filepath):
        if not PDF_AVAILABLE:
            return None
        try:
            with open(filepath, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                pages = [p.extract_text() or "" for p in reader.pages[:50]]
            return "\n".join(pages).strip()
        except Exception:
            return None

    @staticmethod
    def _read_docx(filepath):
        if not DOCX_AVAILABLE:
            return None
        try:
            doc = docx.Document(filepath)
            return "\n".join(p.text for p in doc.paragraphs).strip()
        except Exception:
            return None

    @staticmethod
    def _read_csv(filepath):
        if not CSV_AVAILABLE:
            return None
        for enc in ("utf-8", "latin-1"):
            try:
                return pd.read_csv(filepath, encoding=enc, nrows=500).to_string()
            except Exception:
                continue
        return None

    @staticmethod
    def _read_json(filepath):
        try:
            with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                if filepath.endswith(".jsonl"):
                    lines = [json.loads(ln) for i, ln in enumerate(f) if i < 100]
                    return "\n---\n".join(
                        json.dumps(d, indent=2, ensure_ascii=False) for d in lines
                    )
                return json.dumps(json.load(f), indent=2, ensure_ascii=False)
        except Exception:
            return None

    @staticmethod
    def _read_pptx(filepath):
        if not PPTX_AVAILABLE:
            return None
        try:
            prs = Presentation(filepath)
            texts = []
            for slide in prs.slides[:50]:
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        texts.append(shape.text)
            return "\n".join(texts)
        except Exception:
            return None

    def read_file(self, filepath):
        ext = Path(filepath).suffix.lower()
        readers = {
            ".txt": self._read_txt, ".md": self._read_txt, ".log": self._read_txt,
            ".pdf": self._read_pdf,
            ".docx": self._read_docx, ".doc": self._read_docx,
            ".csv": self._read_csv,
            ".json": self._read_json, ".jsonl": self._read_json,
            ".pptx": self._read_pptx, ".ppt": self._read_pptx,
        }
        reader = readers.get(ext)
        return reader(filepath) if reader else None

    # ‚îÄ‚îÄ –¢—É—Å–ª–∞—Ö —Ñ—É–Ω–∫—Ü—É—É–¥ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @staticmethod
    def _file_hash(filepath):
        try:
            md5 = hashlib.md5()
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(1024 * 1024), b""):
                    md5.update(chunk)
            return md5.hexdigest()
        except Exception:
            return None

    @staticmethod
    def _should_skip(dirpath):
        skip = {
            "node_modules", "__pycache__", ".git", ".venv", "venv",
            "appdata", "windows", "program files", "system32",
            "$recycle.bin", "recovery", "programdata",
        }
        lower = dirpath.lower()
        return any(s in lower for s in skip)

    # ‚îÄ‚îÄ Scan + Index ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def scan_and_index(self, max_files=1000, max_size_mb=10):
        """–§–∞–π–ª—É—É–¥—ã–≥ —É–Ω—à–∏–∂, chunk-–ª—ç–∂, FAISS –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–Ω—ç."""
        print(f"\nüîç –•–∞–π–∂ –±–∞–π–Ω–∞: {', '.join(self.search_paths)}")
        print(f"üìÇ –¢”©—Ä–ª“Ø“Ø–¥: {', '.join(sorted(SUPPORTED_EXTENSIONS))}")

        max_bytes = max_size_mb * 1024 * 1024
        all_chunks = []
        file_count = 0
        file_list = []

        for search_path in self.search_paths:
            if not os.path.exists(search_path):
                print(f"‚ö†Ô∏è –û–ª–¥—Å–æ–Ω–≥“Ø–π: {search_path}")
                continue

            for root, dirs, files in os.walk(search_path):
                if self._should_skip(root):
                    dirs[:] = []
                    continue

                for filename in files:
                    if file_count >= max_files:
                        break

                    ext = Path(filename).suffix.lower()
                    if ext not in SUPPORTED_EXTENSIONS:
                        continue

                    filepath = os.path.join(root, filename)
                    try:
                        fsize = os.path.getsize(filepath)
                        if fsize > max_bytes or fsize == 0:
                            continue
                    except Exception:
                        continue

                    content = self.read_file(filepath)
                    if not content or len(content.strip()) < 50:
                        continue

                    # Chunk-–ª—ç—Ö
                    chunks = self.splitter.split_text(content)

                    file_meta = {
                        "filename": filename,
                        "filepath": filepath,
                        "extension": ext,
                        "size_kb": fsize / 1024,
                        "modified": datetime.fromtimestamp(
                            os.path.getmtime(filepath)
                        ).isoformat(),
                        "hash": self._file_hash(filepath),
                    }
                    file_list.append(file_meta)

                    for ci, chunk_text in enumerate(chunks):
                        doc = Document(
                            page_content=chunk_text,
                            metadata={
                                **file_meta,
                                "chunk_index": ci,
                                "total_chunks": len(chunks),
                            },
                        )
                        all_chunks.append(doc)

                    file_count += 1
                    if file_count % 50 == 0:
                        print(f"  ‚úÖ {file_count} —Ñ–∞–π–ª ({len(all_chunks)} chunk)...")

                if file_count >= max_files:
                    break

        print(f"\nüìä {file_count} —Ñ–∞–π–ª ‚Üí {len(all_chunks)} chunk")

        if not all_chunks:
            print("‚ùå –§–∞–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π")
            return False

        print("üîÑ FAISS –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
        self.db = FAISS.from_documents(all_chunks, self.embedding)
        os.makedirs(INDEX_FOLDER, exist_ok=True)
        self.db.save_local(INDEX_FOLDER)

        self.metadata = {
            "created": datetime.now().isoformat(),
            "num_files": file_count,
            "num_chunks": len(all_chunks),
            "files": file_list,
        }
        with open(METADATA_FILE, "wb") as f:
            pickle.dump(self.metadata, f)

        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞: {INDEX_FOLDER}/")
        return True

    def load_index(self):
        if not os.path.exists(INDEX_FOLDER):
            return False
        try:
            print("üîÑ –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞...")
            self.db = FAISS.load_local(
                INDEX_FOLDER, self.embedding,
                allow_dangerous_deserialization=True,
            )
            if os.path.exists(METADATA_FILE):
                with open(METADATA_FILE, "rb") as f:
                    self.metadata = pickle.load(f)
            nf = self.metadata.get("num_files", self.metadata.get("num_documents", 0))
            nc = self.metadata.get("num_chunks", "?")
            print(f"‚úÖ –ê—á–∞–∞–ª–∞–≥–¥–ª–∞–∞: {nf} —Ñ–∞–π–ª, {nc} chunk")
            return True
        except Exception as e:
            print(f"‚ùå –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞—Ö–∞–¥ –∞–ª–¥–∞–∞: {e}")
            return False

    # ‚îÄ‚îÄ –•–∞–π–ª—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def search(self, query, k=5):
        if not self.db:
            print("‚ùå –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞–∞–≥“Ø–π")
            return []
        try:
            return self.db.similarity_search_with_score(query, k=k)
        except Exception as e:
            print(f"‚ùå –•–∞–π–ª—Ç–∞–¥ –∞–ª–¥–∞–∞: {e}")
            return []

    # ‚îÄ‚îÄ AI —Ö–∞—Ä–∏—É–ª—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def answer(self, question, k=5):
        results = self.search(question, k=k)
        if not results:
            print("‚ùå –•–æ–ª–±–æ–≥–¥–æ—Ö –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π.\n")
            return

        # –•–∞–π–ª—Ç—ã–Ω “Ø—Ä –¥“Ø–Ω —Ö–∞—Ä—É—É–ª–∞—Ö
        seen_files = set()
        sources = []
        context_parts = []

        print(f"\nüìö {len(results)} —Ö–æ–ª–±–æ–≥–¥–æ—Ö —Ö—ç—Å—ç–≥ –æ–ª–¥–ª–æ–æ:")
        for i, (doc, score) in enumerate(results, 1):
            fn = doc.metadata.get("filename", "?")
            ci = doc.metadata.get("chunk_index", 0)
            snippet = doc.page_content[:100].replace("\n", " ")
            print(f"  {i}. {fn} [#{ci}] (score: {score:.3f}) ‚Äî {snippet}...")

            context_parts.append(f"[{fn} —Ö—ç—Å—ç–≥ {ci}]:\n{doc.page_content}")
            if fn not in seen_files:
                sources.append(fn)
                seen_files.add(fn)

        context = "\n\n---\n\n".join(context_parts)

        # AI —Ö–∞—Ä–∏—É–ª—Ç
        if self.ai_mode == "ollama":
            self._answer_ollama(context, question, sources)
        elif self.ai_mode == "flan-t5":
            self._answer_flan(context, question, sources)
        else:
            self._answer_fallback(results, sources)

    def _answer_ollama(self, context, question, sources):
        prompt = f"""–î–∞—Ä–∞–∞—Ö —ç—Ö —Å—É—Ä–≤–∞–ª–∂—É—É–¥–∞–∞—Å –ó”®–í–•”®–ù –º—ç–¥—ç—ç–ª—ç–ª –∞—à–∏–≥–ª–∞–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–Ω–∞ —É—É.

–≠–• –°–£–†–í–ê–õ–ñ–£–£–î:
{context[:3000]}

–ê–°–£–£–õ–¢: {question}

–ó–ê–ê–í–ê–†–ß–ò–õ–ì–ê–ê:
- –ó”©–≤—Ö”©–Ω ”©–≥”©–≥–¥—Å”©–Ω —ç—Ö —Å—É—Ä–≤–∞–ª–∂–∞–∞—Å —Ö–∞—Ä–∏—É–ª–Ω–∞
- –¢–æ–≤—á, —Ç–æ–¥–æ—Ä—Ö–æ–π —Ö–∞—Ä–∏—É–ª–Ω–∞
- –ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π –±–æ–ª "–≠–Ω—ç –º—ç–¥—ç—ç–ª—ç–ª —ç—Ö —Å—É—Ä–≤–∞–ª–∂–∏–¥ –±–∞–π—Ö–≥“Ø–π" –≥—ç–∂ —Ö—ç–ª–Ω—ç
- –ê—Å—É—É–ª—Ç —è–º–∞—Ä —Ö—ç–ª—ç—ç—Ä –±–∞–π–Ω–∞ —Ç—ç—Ä —Ö—ç–ª—ç—ç—Ä —Ö–∞—Ä–∏—É–ª–Ω–∞

–•–ê–†–ò–£–õ–¢:"""

        print(f"\nü§ñ AI ({self.llm.model}):")
        print("‚îÄ" * 50)
        for chunk in self.llm.generate_stream(prompt):
            print(chunk, end="", flush=True)
        print(f"\n\nüìö –≠—Ö: {', '.join(sources)}")
        print("‚îÄ" * 50)

    def _answer_flan(self, context, question, sources):
        prompt = (
            f"Based on these documents, answer concisely:\n\n"
            f"{context[:800]}\n\n"
            f"Question: {question}\nAnswer:"
        )
        if len(prompt.split()) > 400:
            prompt = f"Answer based on:\n{context[:500]}\n\nQ: {question}\nA:"

        try:
            result = self.pipe(
                prompt,
                max_new_tokens=150,
                do_sample=False, num_beams=1, early_stopping=True,
            )
            text = ""
            if isinstance(result, list) and result:
                text = result[0].get("generated_text", "")

            if not text or len(text.strip()) < 5:
                print(f"\nüìã AI –∑–∞–¥–ª–∞–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π.\nüìö –≠—Ö: {', '.join(sources)}")
                return

            text = text.strip()

            # –î–∞–≤—Ç–∞–ª—Ç —à–∞–ª–≥–∞—Ö
            words = text.split()
            if len(words) > 3:
                counts = {}
                for w in words:
                    counts[w] = counts.get(w, 0) + 1
                if max(counts.values()) > len(words) / 3:
                    print(f"\n‚ö†Ô∏è AI –¥–∞–≤—Ç–∞–ª—Ç –∏–ª—ç—Ä—Å—ç–Ω.\nüìö –≠—Ö: {', '.join(sources)}")
                    return

            # Prompt –¥–∞–≤—Ç–∞–ª—Ç —à–∞–ª–≥–∞—Ö
            if "Answer:" in text:
                text = text.split("Answer:")[-1].strip()

            print(f"\nü§ñ AI (flan-t5):")
            print("‚îÄ" * 50)
            print(text)
            print(f"\nüìö –≠—Ö: {', '.join(sources)}")
            print("‚îÄ" * 50)

        except Exception as e:
            print(f"\n‚ö†Ô∏è AI –∞–ª–¥–∞–∞: {e}")

    def _answer_fallback(self, results, sources):
        print(f"\nüìã AI –∏–¥—ç–≤—Ö–≥“Ø–π ‚Äî —Ñ–∞–π–ª—ã–Ω –∞–≥—É—É–ª–≥–∞:")
        print("‚îÄ" * 50)
        for i, (doc, score) in enumerate(results[:3], 1):
            fn = doc.metadata.get("filename", "?")
            print(f"\n[{i}] {fn}:")
            print(doc.page_content[:400].strip())
            if len(doc.page_content) > 400:
                print("...")
        print(f"\nüìö –≠—Ö: {', '.join(sources)}")
        print("‚îÄ" * 50)

    # ‚îÄ‚îÄ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def show_stats(self):
        if not self.metadata:
            print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫ –±–∞–π—Ö–≥“Ø–π\n")
            return
        print("\n" + "=" * 50)
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫")
        print("=" * 50)
        print(f"üìÖ “Æ“Ø—Å—Å—ç–Ω: {self.metadata.get('created', '?')}")
        nf = self.metadata.get("num_files", self.metadata.get("num_documents", 0))
        print(f"üìÑ –§–∞–π–ª: {nf}")
        print(f"üì¶ Chunk: {self.metadata.get('num_chunks', '?')}")
        print(f"ü§ñ AI: {self.ai_mode}"
              + (f" ({self.llm.model})" if self.ai_mode == "ollama" else ""))

        if "files" in self.metadata:
            exts = {}
            total_kb = 0.0
            for fm in self.metadata["files"]:
                ext = fm.get("extension", "?")
                exts[ext] = exts.get(ext, 0) + 1
                total_kb += fm.get("size_kb", 0)
            print(f"üíæ –•—ç–º–∂—ç—ç: {total_kb / 1024:.1f} MB")
            print("\nüìÇ –¢”©—Ä–ª“Ø“Ø–¥:")
            for ext, cnt in sorted(exts.items(), key=lambda x: -x[1]):
                print(f"   {ext}: {cnt}")
            print(f"\nüìã –§–∞–π–ª—É—É–¥ ({min(20, len(self.metadata['files']))}"
                  f"/{len(self.metadata['files'])}):")
            for i, fm in enumerate(self.metadata["files"][:20], 1):
                print(f"   {i}. {fm.get('filename')} ({fm.get('extension')})")
            if len(self.metadata["files"]) > 20:
                print(f"   ... +{len(self.metadata['files']) - 20}")
        print("=" * 50 + "\n")

    # ‚îÄ‚îÄ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤ –≥–æ—Ä–∏–º ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def interactive(self):
        print("\n" + "=" * 50)
        print("üß† RAG –•–∞–π–ª—Ç—ã–Ω –°–∏—Å—Ç–µ–º")
        print("=" * 50)
        ai_label = self.ai_mode
        if self.ai_mode == "ollama":
            ai_label += f" ({self.llm.model})"
        print(f"ü§ñ AI: {ai_label}")
        print("üí° –ö–æ–º–∞–Ω–¥—É—É–¥: stats | rescan | model <–Ω—ç—Ä> | exit")
        print("=" * 50 + "\n")

        while True:
            try:
                q = input("‚ùì –ê—Å—É—É–ª—Ç: ").strip()
            except (KeyboardInterrupt, EOFError):
                print("\nüëã –ë–∞—è—Ä—Ç–∞–π!")
                break

            if not q:
                continue

            cmd = q.lower()
            if cmd == "exit":
                print("üëã –ë–∞—è—Ä—Ç–∞–π!")
                break
            if cmd == "stats":
                self.show_stats()
                continue
            if cmd == "rescan":
                self.scan_and_index()
                continue
            if cmd.startswith("model "):
                if self.ai_mode == "ollama":
                    self.llm.model = q[6:].strip()
                    print(f"‚úÖ Model: {self.llm.model}\n")
                else:
                    print("‚ö†Ô∏è Ollama –∏–¥—ç–≤—Ö–≥“Ø–π –±–∞–π–Ω–∞\n")
                continue

            self.answer(q)
            print()


def main():
    print("üöÄ RAG –î–∏—Å–∫ –•–∞–π–ª—Ç—ã–Ω –°–∏—Å—Ç–µ–º\n")

    # –°—É—É–≥–∞–∞–≥“Ø–π —Å–∞–Ω–≥—É—É–¥—ã–Ω –º—ç–¥—ç–≥–¥—ç–ª
    missing = []
    if not PDF_AVAILABLE:
        missing.append("PyPDF2")
    if not DOCX_AVAILABLE:
        missing.append("python-docx")
    if not CSV_AVAILABLE:
        missing.append("pandas")
    if not PPTX_AVAILABLE:
        missing.append("python-pptx")
    if missing:
        print(f"‚ö†Ô∏è –°—É—É–≥–∞–∞–≥“Ø–π —Å–∞–Ω–≥—É—É–¥: {', '.join(missing)}")
        print(f"   pip install {' '.join(missing)}\n")

    # –•–∞–π—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏
    print("üìÅ –•–∞–π—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏ (—Ç–∞—Å–ª–∞–ª–∞–∞—Ä —Ç—É—Å–≥–∞–∞—Ä–ª–∞–Ω–∞):")
    print("   –ñ–∏—à—ç—ç: D:/Documents, D:/Projects")
    try:
        user_paths = input("\nüìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏: ").strip()
    except (KeyboardInterrupt, EOFError):
        return

    paths = (
        [p.strip() for p in user_paths.split(",") if p.strip()]
        if user_paths
        else ["D:/", "C:/Users"]
    )

    rag = DiskSearchRAG(search_paths=paths)

    # –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞—Ö —ç—Å–≤—ç–ª —à–∏–Ω—ç—ç—Ä “Ø“Ø—Å–≥—ç—Ö
    if os.path.exists(INDEX_FOLDER):
        print(f"\n‚úÖ –ò–Ω–¥–µ–∫—Å –æ–ª–¥–ª–æ–æ: {INDEX_FOLDER}")
        try:
            choice = input("–ê—à–∏–≥–ª–∞—Ö —É—É? (y/n, default=y): ").strip().lower()
        except (KeyboardInterrupt, EOFError):
            return
        if choice != "n":
            if rag.load_index():
                rag.interactive()
                return

    print("\nüîÑ –®–∏–Ω—ç –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
    try:
        max_f = input("–§–∞–π–ª—ã–Ω –¥—ç—ç–¥ —Ö—è–∑–≥–∞–∞—Ä (default 1000): ").strip()
    except (KeyboardInterrupt, EOFError):
        return
    max_files = int(max_f) if max_f.isdigit() else 1000

    if rag.scan_and_index(max_files=max_files):
        rag.interactive()
    else:
        print("‚ùå –ò–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π")


if __name__ == "__main__":
    main()