import os
import json
import pickle
import requests
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from tqdm import tqdm
import hashlib

# –§–∞–π–ª —É–Ω—à–∏–≥—á —Å–∞–Ω–≥—É—É–¥
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("‚ö†Ô∏è PyPDF2 —Å—É—É–≥–∞–∞–≥“Ø–π: pip install PyPDF2")

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("‚ö†Ô∏è python-docx —Å—É—É–≥–∞–∞–≥“Ø–π: pip install python-docx")

try:
    import pandas as pd
    CSV_AVAILABLE = True
except ImportError:
    CSV_AVAILABLE = False
    print("‚ö†Ô∏è pandas —Å—É—É–≥–∞–∞–≥“Ø–π: pip install pandas")

try:
    from pptx import Presentation
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False
    print("‚ö†Ô∏è python-pptx —Å—É—É–≥–∞–∞–≥“Ø–π: pip install python-pptx")

load_dotenv()

# === CONFIG ===
EMBEDDING_MODEL = "sentence-transformers/all-distilroberta-v1"
INDEX_FOLDER = "faiss_disk_index"
METADATA_FILE = "disk_metadata.pkl"
SUPPORTED_EXTENSIONS = ['.txt', '.pdf', '.docx', '. doc', '.json', '.jsonl', '.csv', '.md', '.pptx', '.ppt']

# === OLLAMA CONFIG ===
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "qwen2"  # –¢–∞–Ω—ã —Ç–∞—Ç—Å–∞–Ω model


class OllamaLLM:
    """Ollama LLM wrapper"""
    
    def __init__(self, model: str = OLLAMA_MODEL, base_url: str = OLLAMA_BASE_URL):
        self.model = model
        self. base_url = base_url
        self.api_url = f"{base_url}/api/generate"
        
    def is_available(self) -> bool:
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def list_models(self) -> list:
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [m["name"] for m in data.get("models", [])]
            return []
        except:
            return []
    
    def generate(self, prompt: str, temperature: float = 0.3, max_tokens: int = 1024) -> str:
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                }
            }
            
            # 10 –º–∏–Ω—É—Ç timeout
            response = requests.post(self.api_url, json=payload, timeout=600)
            
            if response.status_code == 200:
                data = response.json()
                return data.get("response", ""). strip()
            else:
                return f"Ollama error: {response.status_code}"
                
        except requests.exceptions. ReadTimeout:
            return "‚ùå –•–∞—Ä–∏—É–ª—Ç —É–¥–∞–∂ –±–∞–π–Ω–∞ (10 –º–∏–Ω—É—Ç–∞–∞—Å –∏–ª“Ø“Ø).  –ñ–∏–∂–∏–≥ model –∞—à–∏–≥–ª–∞–Ω–∞ —É—É."
        except requests.exceptions.ConnectionError:
            return "‚ùå Ollama —Ö–æ–ª–±–æ–≥–¥–æ–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π."
        except Exception as e:
            return f"‚ùå –ê–ª–¥–∞–∞: {e}"
    
    def generate_stream(self, prompt: str, temperature: float = 0.3):
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": temperature,
                }
            }
            
            # 10 –º–∏–Ω—É—Ç timeout
            response = requests.post(self.api_url, json=payload, stream=True, timeout=600)
            
            for line in response. iter_lines():
                if line:
                    data = json. loads(line)
                    if "response" in data:
                        yield data["response"]
                    if data.get("done", False):
                        break
                        
        except requests.exceptions. ReadTimeout:
            yield "‚ùå –•–∞—Ä–∏—É–ª—Ç —É–¥–∞–∂ –±–∞–π–Ω–∞."
        except Exception as e:
            yield f"‚ùå –ê–ª–¥–∞–∞: {e}"


class AdvancedDiskSearch:
    def __init__(self, search_paths=None):
        self.search_paths = search_paths or ["D:/", "C:/Users"]
        self.embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        self. db = None
        self.metadata = {}
        self.all_docs = []
        
        # Initialize Ollama instead of flan-t5
        print("üîÑ Ollama —Ö–æ–ª–±–æ–≥–¥–æ–∂ –±–∞–π–Ω–∞...")
        self.llm = OllamaLLM(model=OLLAMA_MODEL)
        
        if self.llm.is_available():
            models = self.llm.list_models()
            print(f"‚úÖ Ollama –±—ç–ª—ç–Ω!  –ú–æ–¥–µ–ª—É—É–¥: {', '.join(models[:5])}")
            
            # Check if selected model exists
            model_names = [m. split(":")[0] for m in models]
            if OLLAMA_MODEL not in model_names and OLLAMA_MODEL. split(":")[0] not in model_names:
                print(f"‚ö†Ô∏è '{OLLAMA_MODEL}' model –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
                if models:
                    self.llm.model = models[0]
                    print(f"   '{self.llm.model}' model –∞—à–∏–≥–ª–∞–∂ –±–∞–π–Ω–∞")
        else:
            print("‚ùå Ollama –∞–∂–∏–ª–ª–∞—Ö–≥“Ø–π –±–∞–π–Ω–∞!")
            print("   1. Ollama —Å—É—É–ª–≥–∞—Ö: https://ollama.ai")
            print("   2.  –ê–∂–∏–ª–ª—É—É–ª–∞—Ö: ollama serve")
            print("   3. Model —Ç–∞—Ç–∞—Ö: ollama pull qwen2")

    def get_file_hash(self, filepath):
        try:
            with open(filepath, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return None

    def should_skip_directory(self, dirpath):
        skip_dirs = {
            'node_modules', '__pycache__', '.git', '.venv', 'venv',
            'AppData', 'Windows', 'Program Files', 'System32',
            '$RECYCLE.BIN', 'Recovery', 'ProgramData',
            'Microsoft VS Code', 'Visual Studio', 'extensions',
            'resources', 'locales', 'vendor', 'build', 'dist'
        }
        return any(skip in dirpath for skip in skip_dirs)

    def read_txt_file(self, filepath):
        encodings = ['utf-8', 'utf-16', 'cp1252', 'latin-1']
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    return f.read()
            except:
                continue
        return None

    def read_pdf_file(self, filepath):
        if not PDF_AVAILABLE:
            return None
        try:
            text = ""
            with open(filepath, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            return text. strip()
        except Exception as e:
            print(f"‚ö†Ô∏è PDF —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_docx_file(self, filepath):
        if not DOCX_AVAILABLE:
            return None
        try:
            doc = docx. Document(filepath)
            text = "\n".join([para.text for para in doc.paragraphs])
            return text. strip()
        except Exception as e:
            print(f"‚ö†Ô∏è DOCX —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_csv_file(self, filepath):
        if not CSV_AVAILABLE:
            return None
        try:
            df = pd.read_csv(filepath, encoding='utf-8', nrows=1000)
            return df.to_string()
        except Exception as e:
            try:
                df = pd.read_csv(filepath, encoding='latin-1', nrows=1000)
                return df.to_string()
            except:
                print(f"‚ö†Ô∏è CSV —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
                return None

    def read_json_file(self, filepath):
        try:
            if filepath.endswith('.jsonl'):
                texts = []
                with open(filepath, 'r', encoding='utf-8') as f:
                    for line in f:
                        data = json.loads(line)
                        texts.append(json.dumps(data, indent=2, ensure_ascii=False))
                return "\n---\n".join(texts[:100])
            else:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return json.dumps(data, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"‚ö†Ô∏è JSON —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_pptx_file(self, filepath):
        if not PPTX_AVAILABLE:
            return None
        try:
            prs = Presentation(filepath)
            text = []
            for slide in prs. slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text.append(shape.text)
            return "\n". join(text)
        except Exception as e:
            print(f"‚ö†Ô∏è PPTX —É–Ω—à–∏—Ö–∞–¥ –∞–ª–¥–∞–∞ {filepath}: {e}")
            return None

    def read_file(self, filepath):
        ext = Path(filepath).suffix.lower()
        if ext in ['.txt', '.md', '.log']:
            return self.read_txt_file(filepath)
        elif ext == '.pdf':
            return self. read_pdf_file(filepath)
        elif ext in ['.docx', '.doc']:
            return self.read_docx_file(filepath)
        elif ext == '.csv':
            return self.read_csv_file(filepath)
        elif ext in ['.json', '.jsonl']:
            return self.read_json_file(filepath)
        elif ext in ['.pptx', '.ppt']:
            return self.read_pptx_file(filepath)
        else:
            return None

    def scan_disk(self, max_files=1000, max_size_mb=10):
        print(f"üîç –î–∏—Å–∫ —Ö–∞–π–∂ —ç—Ö—ç–ª–∂ –±–∞–π–Ω–∞: {self.search_paths}")
        print(f"üìÇ –î—ç–º–∂–∏–≥–¥—Å—ç–Ω —Ñ–∞–π–ª—ã–Ω —Ç”©—Ä”©–ª: {', '.join(SUPPORTED_EXTENSIONS)}")
        documents = []
        file_count = 0
        max_size_bytes = max_size_mb * 1024 * 1024
        for search_path in self.search_paths:
            if not os.path.exists(search_path):
                print(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏ –æ–ª–¥—Å–æ–Ω–≥“Ø–π: {search_path}")
                continue
            print(f"\nüìÅ –•–∞–π–∂ –±–∞–π–Ω–∞: {search_path}")
            for root, dirs, files in os.walk(search_path):
                if self.should_skip_directory(root):
                    dirs[:] = []
                    continue
                for filename in files:
                    if file_count >= max_files:
                        print(f"\n‚ö†Ô∏è –•—è–∑–≥–∞–∞—Ä—Ç —Ö“Ø—Ä–ª—ç—ç: {max_files} —Ñ–∞–π–ª")
                        break
                    filepath = os.path.join(root, filename)
                    ext = Path(filename).suffix.lower()
                    if ext not in SUPPORTED_EXTENSIONS:
                        continue
                    try:
                        file_size = os.path.getsize(filepath)
                        if file_size > max_size_bytes or file_size == 0:
                            continue
                    except:
                        continue
                    content = self.read_file(filepath)
                    if not content or len(content. strip()) < 50:
                        continue
                    file_hash = self.get_file_hash(filepath)
                    modified_time = datetime.fromtimestamp(os. path.getmtime(filepath))
                    doc = Document(
                        page_content=content[:5000],
                        metadata={
                            "filename": filename,
                            "filepath": filepath,
                            "extension": ext,
                            "size_kb": file_size / 1024,
                            "modified": modified_time.isoformat(),
                            "hash": file_hash
                        }
                    )
                    documents.append(doc)
                    file_count += 1
                    if file_count % 50 == 0:
                        print(f"‚úÖ {file_count} —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω...")
                if file_count >= max_files:
                    break
        print(f"\n‚úÖ –ù–∏–π—Ç {len(documents)} —Ñ–∞–π–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω")
        self.all_docs = documents
        return documents

    def create_index(self, documents):
        if not documents:
            print("‚ùå –ò–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç—Ö –±–∞—Ä–∏–º—Ç –±–∞–π—Ö–≥“Ø–π")
            return False
        print(f"\nüîÑ FAISS –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞ ({len(documents)} –±–∞—Ä–∏–º—Ç)...")
        try:
            self.db = FAISS.from_documents(documents, self.embedding)
            os.makedirs(INDEX_FOLDER, exist_ok=True)
            self.db.save_local(INDEX_FOLDER)
            self.metadata = {
                "created": datetime.now().isoformat(),
                "num_documents": len(documents),
                "files": [doc.metadata for doc in documents]
            }
            with open(METADATA_FILE, 'wb') as f:
                pickle.dump(self.metadata, f)
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∞–º–∂–∏–ª—Ç—Ç–∞–π “Ø“Ø—Å–≥—ç–≥–¥–ª—ç—ç: {INDEX_FOLDER}")
            return True
        except Exception as e:
            print(f"‚ùå –ò–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç—Ö—ç–¥ –∞–ª–¥–∞–∞: {e}")
            return False

    def load_index(self):
        if not os.path.exists(INDEX_FOLDER):
            print("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –æ–ª–¥—Å–æ–Ω–≥“Ø–π.  –≠—Ö–ª—ç—ç–¥ scan_disk() –¥—É—É–¥–Ω–∞ —É—É.")
            return False
        try:
            print("üîÑ –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞...")
            self.db = FAISS.load_local(INDEX_FOLDER, self. embedding, allow_dangerous_deserialization=True)
            if os.path.exists(METADATA_FILE):
                with open(METADATA_FILE, 'rb') as f:
                    self. metadata = pickle.load(f)
                print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞–≥–¥–ª–∞–∞: {self.metadata. get('num_documents', 0)} –±–∞—Ä–∏–º—Ç")
            else:
                print("‚ö†Ô∏è Metadata –æ–ª–¥—Å–æ–Ω–≥“Ø–π")
            return True
        except Exception as e:
            print(f"‚ùå –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞—Ö–∞–¥ –∞–ª–¥–∞–∞: {e}")
            return False

    def search_by_keyword(self, keyword):
        keyword = keyword.lower()
        results = []
        if self.metadata and "files" in self.metadata:
            for file_meta in self.metadata["files"]:
                if keyword in file_meta.get("filename", "").lower():
                    results.append(file_meta)
        return results

    def semantic_search(self, query, k=5, score_threshold=2.0):
        if not self.db:
            print("‚ùå –ò–Ω–¥–µ–∫—Å –∞—á–∞–∞–ª–∞–∞–≥“Ø–π –±–∞–π–Ω–∞")
            return []
        try:
            results = self.db.similarity_search_with_score(query, k=k)
            filtered = [(doc, score) for doc, score in results if score < score_threshold]
            if not filtered and results:
                print(f"‚ö†Ô∏è Threshold-–æ–æ—Å –¥–∞–≤—Å–∞–Ω, –±“Ø—Ö “Ø—Ä –¥“Ø–Ω–≥ —Ö–∞—Ä—É—É–ª–∂ –±–∞–π–Ω–∞")
                filtered = results
            return filtered
        except Exception as e:
            print(f"‚ùå –•–∞–π–ª—Ç–∞–¥ –∞–ª–¥–∞–∞: {e}")
            return []

    def generate_answer(self, results, question, stream=True):
        """
        Ollama –∞—à–∏–≥–ª–∞–Ω —Ö–∞—Ä–∏—É–ª—Ç “Ø“Ø—Å–≥—ç—Ö
        """
        if not results:
            return "–•–æ–ª–±–æ–≥–¥–æ—Ö –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π."
        
        if not self.llm.is_available():
            return "‚ùå Ollama –∞–∂–∏–ª–ª–∞—Ö–≥“Ø–π –±–∞–π–Ω–∞.  'ollama serve' –∞–∂–∏–ª–ª—É—É–ª–Ω–∞ —É—É."
        
        # Build context from search results
        snippets = []
        sources = []
        
        for i, (doc, score) in enumerate(results, 1):
            filename = doc.metadata.get("filename", f"source_{i}")
            filepath = doc.metadata.get("filepath", "")
            content = doc.page_content[:1500]. replace("\n", " ").strip()
            
            snippets.append(f"""
üìÑ –≠—Ö —Å—É—Ä–≤–∞–ª–∂ {i}: {filename}
üìÇ –ë–∞–π—Ä—à–∏–ª: {filepath}
üìù –ê–≥—É—É–ª–≥–∞:
{content}
""")
            sources.append(filename)
        
        context = "\n---\n".join(snippets)
        
        # Create prompt
        prompt = f"""–¢–∞ —Ç—É—Å–ª–∞—Ö AI –±–∞–π–Ω–∞.  –î–∞—Ä–∞–∞—Ö —ç—Ö —Å—É—Ä–≤–∞–ª–∂—É—É–¥–∞–∞—Å –ó”®–í–•”®–ù –º—ç–¥—ç—ç–ª—ç–ª –∞—à–∏–≥–ª–∞–Ω –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–Ω–∞ —É—É. 

üìö –≠–• –°–£–†–í–ê–õ–ñ–£–£–î:
{context}

‚ùì –ê–°–£–£–õ–¢: {question}

üìã –ó–ê–ê–í–ê–†–ß–ò–õ–ì–ê–ê:
1.  –ó”©–≤—Ö”©–Ω ”©–≥”©–≥–¥—Å”©–Ω —ç—Ö —Å—É—Ä–≤–∞–ª–∂–∞–∞—Å —Ö–∞—Ä–∏—É–ª–Ω–∞
2. –≠—Ö —Å—É—Ä–≤–∞–ª–∂–∏–π–≥ [filename] —Ö—ç–ª–±—ç—Ä—ç—ç—Ä –¥—É—Ä–¥–∞–Ω–∞
3. –•—ç—Ä—ç–≤ –º—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π –±–æ–ª "–≠–Ω—ç –º—ç–¥—ç—ç–ª—ç–ª —ç—Ö —Å—É—Ä–≤–∞–ª–∂–∏–¥ –±–∞–π—Ö–≥“Ø–π –±–∞–π–Ω–∞" –≥—ç–∂ —Ö—ç–ª–Ω—ç
4. –¢–æ–≤—á, —Ç–æ–¥–æ—Ä—Ö–æ–π —Ö–∞—Ä–∏—É–ª–Ω–∞
5. –ê—Å—É—É–ª—Ç —è–º–∞—Ä —Ö—ç–ª—ç—ç—Ä –±–∞–π–Ω–∞ —Ç—ç—Ä —Ö—ç–ª—ç—ç—Ä —Ö–∞—Ä–∏—É–ª–Ω–∞

‚úçÔ∏è –•–ê–†–ò–£–õ–¢:"""

        if stream:
            # Streaming response
            print("\nüí° AI –•–∞—Ä–∏—É–ª—Ç:")
            full_response = ""
            for chunk in self.llm. generate_stream(prompt, temperature=0.3):
                print(chunk, end="", flush=True)
                full_response += chunk
            print()  # New line after streaming
            
            # Add sources if not mentioned
            if not any(s in full_response for s in sources):
                sources_text = f"\n\nüìö –ê—à–∏–≥–ª–∞—Å–∞–Ω —ç—Ö —Å—É—Ä–≤–∞–ª–∂: {', '.join(sources)}"
                print(sources_text)
                full_response += sources_text
            
            return full_response
        else:
            # Non-streaming response
            response = self.llm.generate(prompt, temperature=0.3, max_tokens=1024)
            
            if not any(s in response for s in sources):
                response += f"\n\nüìö –ê—à–∏–≥–ª–∞—Å–∞–Ω —ç—Ö —Å—É—Ä–≤–∞–ª–∂: {', '.join(sources)}"
            
            return response

    def interactive_search(self):
        print("\n" + "="*60)
        print("üß† –î–∏—Å–∫ –•–∞–π–ª—Ç—ã–Ω AI –°–∏—Å—Ç–µ–º (Ollama)")
        print("="*60)
        print(f"ü§ñ Model: {self.llm.model}")
        print("üí° –ö–æ–º–∞–Ω–¥—É—É–¥:")
        print("  - 'stats' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ —Ö–∞—Ä–∞—Ö")
        print("  - 'rescan' - –¥–∞—Ö–∏–Ω —Ö–∞–π—Ö")
        print("  - 'models' - –º–æ–¥–µ–ª—É—É–¥ —Ö–∞—Ä–∞—Ö")
        print("  - 'model <name>' - –º–æ–¥–µ–ª —Å–æ–ª–∏—Ö")
        print("  - 'exit' - –≥–∞—Ä–∞—Ö")
        print("="*60 + "\n")

        while True:
            try:
                user_input = input("üîç –ê—Å—É—É–ª—Ç: ").strip()
            except (KeyboardInterrupt, EOFError):
                print("\nüëã –ë–∞—è—Ä—Ç–∞–π!")
                break
                
            if not user_input:
                continue
            if user_input.lower() == "exit":
                print("üëã –ë–∞—è—Ä—Ç–∞–π!")
                break
            if user_input.lower() == "stats":
                self.show_statistics()
                continue
            if user_input.lower() == "models":
                models = self.llm.list_models()
                print(f"\nüìã –ë–æ–ª–æ–º–∂–∏—Ç –º–æ–¥–µ–ª—É—É–¥: {', '.join(models)}")
                print(f"üîπ –û–¥–æ–æ–≥–∏–π–Ω –º–æ–¥–µ–ª: {self.llm.model}\n")
                continue
            if user_input.lower(). startswith("model "):
                new_model = user_input[6:].strip()
                self.llm.model = new_model
                print(f"‚úÖ –ú–æ–¥–µ–ª —Å–æ–ª–∏–≥–¥–ª–æ–æ: {new_model}\n")
                continue
            if user_input.lower() == "rescan":
                print("\nüîÑ –î–∞—Ö–∏–Ω —Ö–∞–π–∂, –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
                docs = self.scan_disk(max_files=500)
                self.create_index(docs)
                continue

            # –¢“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥—ç—ç—Ä —Ö–∞–π–ª—Ç
            keyword_results = self.search_by_keyword(user_input)
            if keyword_results:
                print(f"\n[üîé –¢“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥] {len(keyword_results)} —Ñ–∞–π–ª –æ–ª–¥–ª–æ–æ:")
                for result in keyword_results[:3]:
                    print(f"  üìÑ {result['filename']} ({result['extension']})")
                    print(f"     üìÅ {result['filepath']}")

            # –£—Ç–≥—ã–Ω —Ö–∞–π–ª—Ç
            semantic_results = self.semantic_search(user_input, k=5)
            if not semantic_results:
                print("\n‚ùå –•–æ–ª–±–æ–≥–¥–æ—Ö –º—ç–¥—ç—ç–ª—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
                print("üí° –ó”©–≤–ª”©–º–∂:")
                print("   - ”®”©—Ä “Ø–≥ —Ö—ç–ª–ª—ç–≥—ç—ç—Ä –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É")
                print("   - 'stats' –∫–æ–º–∞–Ω–¥–∞–∞—Ä —è–º–∞—Ä —Ñ–∞–π–ª—É—É–¥ –±–∞–π–≥–∞–∞–≥ —Ö–∞—Ä–Ω–∞ —É—É")
                print("   - 'rescan' –∫–æ–º–∞–Ω–¥–∞–∞—Ä –¥–∞—Ö–∏–Ω —Ö–∞–π–∂ “Ø–∑–Ω—ç “Ø“Ø\n")
                continue

            print(f"\n[üìö –£—Ç–≥—ã–Ω —Ö–∞–π–ª—Ç] {len(semantic_results)} –±–∞—Ä–∏–º—Ç –æ–ª–¥–ª–æ–æ:")
            for i, (doc, score) in enumerate(semantic_results, 1):
                snippet = doc.page_content[:200].replace("\n", " ") + "..."
                print(f"\n{i}. üéØ –û–Ω–æ–æ: {score:.4f}")
                print(f"   üìÑ –§–∞–π–ª: {doc.metadata. get('filename', 'Unknown')}")
                print(f"   üìÇ –ó–∞–º: {doc.metadata.get('filepath', 'Unknown')}")
                print(f"   üìè –•—ç–º–∂—ç—ç: {doc.metadata.get('size_kb', 0):.1f} KB")
                print(f"   üìù –ê–≥—É—É–ª–≥–∞: {snippet}")

            # Generate answer with Ollama (streaming)
            self.generate_answer(semantic_results[:3], user_input, stream=True)
            
            print("\n" + "-" * 60)

    def show_statistics(self):
        if not self.metadata:
            print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫ –±–∞–π—Ö–≥“Ø–π")
            return
        print("\n" + "="*60)
        print("üìä –°–∏—Å—Ç–µ–º–∏–π–Ω –°—Ç–∞—Ç–∏—Å—Ç–∏–∫")
        print("="*60)
        print(f"üìÖ “Æ“Ø—Å—Å—ç–Ω: {self.metadata. get('created', 'Unknown')}")
        print(f"üìÑ –ù–∏–π—Ç –±–∞—Ä–∏–º—Ç: {self.metadata.get('num_documents', 0)}")
        print(f"ü§ñ Ollama Model: {self.llm.model}")
        print(f"üîå Ollama Status: {'‚úÖ –ê–∂–∏–ª–ª–∞–∂ –±–∞–π–Ω–∞' if self.llm.is_available() else '‚ùå –ê–∂–∏–ª–ª–∞—Ö–≥“Ø–π'}")
        
        if "files" in self.metadata:
            extensions = {}
            total_size = 0
            for file_meta in self.metadata["files"]:
                ext = file_meta.get("extension", "unknown")
                size = file_meta.get("size_kb", 0)
                extensions[ext] = extensions.get(ext, 0) + 1
                total_size += size
            print(f"üíæ –ù–∏–π—Ç —Ö—ç–º–∂—ç—ç: {total_size/1024:.2f} MB")
            print("\nüìÇ –§–∞–π–ª—ã–Ω —Ç”©—Ä”©–ª:")
            for ext, count in sorted(extensions.items(), key=lambda x: x[1], reverse=True):
                print(f"   {ext}: {count} —Ñ–∞–π–ª")
            print("\nüìã –ë–∞—Ä–∏–º—Ç—ã–Ω –∂–∞–≥—Å–∞–∞–ª—Ç:")
            for i, file_meta in enumerate(self.metadata["files"][:20], 1):
                print(f"   {i}. {file_meta. get('filename', 'Unknown')} ({file_meta.get('extension', 'unknown')})")
            if len(self.metadata["files"]) > 20:
                print(f"   ... –±–æ–ª–æ–Ω ”©”©—Ä {len(self.metadata['files']) - 20} —Ñ–∞–π–ª")
        print("="*60 + "\n")


def main():
    print("üöÄ –î–∏—Å–∫ –•–∞–π–ª—Ç—ã–Ω –°–∏—Å—Ç–µ–º (Ollama)\n")
    
    # Check Ollama
    print("üîç Ollama —à–∞–ª–≥–∞–∂ –±–∞–π–Ω–∞...")
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = [m["name"] for m in response.json(). get("models", [])]
            print(f"‚úÖ Ollama –±—ç–ª—ç–Ω!  –ú–æ–¥–µ–ª—É—É–¥: {', '.join(models[:5])}")
        else:
            print("‚ö†Ô∏è Ollama —Ö–∞—Ä–∏—É ”©–≥”©—Ö–≥“Ø–π –±–∞–π–Ω–∞")
    except:
        print("‚ùå Ollama –∞–∂–∏–ª–ª–∞—Ö–≥“Ø–π –±–∞–π–Ω–∞!")
        print("\nüìã –ó–∞—Å–∞—Ö –∑–∞–∞–≤–∞—Ä:")
        print("   1. –®–∏–Ω—ç PowerShell –Ω—ç—ç–∂: ollama serve")
        print("   2.  Model —Ç–∞—Ç—Å–∞–Ω —ç—Å—ç—Ö: ollama list")
        print("   3. Model —Ç–∞—Ç–∞–∞–≥“Ø–π –±–æ–ª: ollama pull qwen2")
        input("\nEnter –¥–∞—Ä–∂ “Ø—Ä–≥—ç–ª–∂–ª“Ø“Ø–ª—ç—Ö...")
    
    print("\nüìÅ –•–∞–π—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—É—É–¥—ã–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É (—Ç–∞—Å–ª–∞–ª–∞–∞—Ä —Ç—É—Å–≥–∞–∞—Ä–ª–∞–Ω–∞):")
    print("   –ñ–∏—à—ç—ç: D:/Documents, D:/Projects, C:/Users/YourName/Desktop")
    print("   –•–æ–æ—Å–æ–Ω –æ—Ä—Ö–∏–≤–æ–ª D:/ –±–æ–ª–æ–Ω C:/Users —Ö–∞–π–Ω–∞")
    user_paths = input("\nüìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏: ").strip()
    if user_paths:
        search_paths = [p.strip() for p in user_paths.split(",")]
    else:
        search_paths = ["D:/", "C:/Users"]
    searcher = AdvancedDiskSearch(search_paths=search_paths)
    if os.path.exists(INDEX_FOLDER):
        print(f"\n‚úÖ –•–∞–¥–≥–∞–ª—Å–∞–Ω –∏–Ω–¥–µ–∫—Å –æ–ª–¥–ª–æ–æ: {INDEX_FOLDER}")
        choice = input("–ê—à–∏–≥–ª–∞—Ö —É—É? (y/n): ").strip().lower()
        if choice == 'y':
            if searcher.load_index():
                searcher.interactive_search()
                return
    print("\nüîÑ –î–∏—Å–∫ —Ö–∞–π–∂, –∏–Ω–¥–µ–∫—Å “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...")
    print("‚ö†Ô∏è –≠–Ω—ç —É–¥–∞–∞–Ω “Ø—Ä–≥—ç–ª–∂–ª—ç—Ö –±–æ–ª–æ–º–∂—Ç–æ–π (5-30 –º–∏–Ω—É—Ç)")
    max_files = input("\n–•–∞–º–≥–∏–π–Ω –∏—Ö —Ñ–∞–π–ª—ã–Ω —Ç–æ–æ (default 1000): ").strip()
    max_files = int(max_files) if max_files. isdigit() else 1000
    docs = searcher.scan_disk(max_files=max_files, max_size_mb=10)
    if docs:
        if searcher.create_index(docs):
            searcher.interactive_search()
    else:
        print("‚ùå –§–∞–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π")


if __name__ == "__main__":
    main()